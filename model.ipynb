{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras import optimizers\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"pass_5k.txt\"\n",
    "raw_text = open(filename,encoding=\"utf8\").read()\n",
    "raw_text = raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " '!': 1,\n",
       " '$': 2,\n",
       " '%': 3,\n",
       " '(': 4,\n",
       " ')': 5,\n",
       " '+': 6,\n",
       " '-': 7,\n",
       " '.': 8,\n",
       " '0': 9,\n",
       " '1': 10,\n",
       " '2': 11,\n",
       " '3': 12,\n",
       " '4': 13,\n",
       " '5': 14,\n",
       " '6': 15,\n",
       " '7': 16,\n",
       " '8': 17,\n",
       " '9': 18,\n",
       " '@': 19,\n",
       " 'A': 20,\n",
       " 'B': 21,\n",
       " 'C': 22,\n",
       " 'D': 23,\n",
       " 'E': 24,\n",
       " 'F': 25,\n",
       " 'G': 26,\n",
       " 'H': 27,\n",
       " 'I': 28,\n",
       " 'J': 29,\n",
       " 'K': 30,\n",
       " 'L': 31,\n",
       " 'M': 32,\n",
       " 'N': 33,\n",
       " 'O': 34,\n",
       " 'P': 35,\n",
       " 'Q': 36,\n",
       " 'R': 37,\n",
       " 'S': 38,\n",
       " 'T': 39,\n",
       " 'U': 40,\n",
       " 'V': 41,\n",
       " 'W': 42,\n",
       " 'X': 43,\n",
       " 'Y': 44,\n",
       " 'Z': 45,\n",
       " '_': 46,\n",
       " 'a': 47,\n",
       " 'b': 48,\n",
       " 'c': 49,\n",
       " 'd': 50,\n",
       " 'e': 51,\n",
       " 'f': 52,\n",
       " 'g': 53,\n",
       " 'h': 54,\n",
       " 'i': 55,\n",
       " 'j': 56,\n",
       " 'k': 57,\n",
       " 'l': 58,\n",
       " 'm': 59,\n",
       " 'n': 60,\n",
       " 'o': 61,\n",
       " 'p': 62,\n",
       " 'q': 63,\n",
       " 'r': 64,\n",
       " 's': 65,\n",
       " 't': 66,\n",
       " 'u': 67,\n",
       " 'v': 68,\n",
       " 'w': 69,\n",
       " 'x': 70,\n",
       " 'y': 71,\n",
       " 'z': 72}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  47497\n",
      "Total Vocab:  73\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  47472\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 25\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47472, 25, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"Bigger_Model_weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "47472/47472 [==============================] - 42s 890us/step - loss: 3.0128\n",
      "\n",
      "Epoch 00001: loss improved from 3.03229 to 3.01285, saving model to Bigger_Model_weights-improvement-01-3.0128.hdf5\n",
      "Epoch 2/500\n",
      "47472/47472 [==============================] - 42s 888us/step - loss: 2.9993\n",
      "\n",
      "Epoch 00002: loss improved from 3.01285 to 2.99927, saving model to Bigger_Model_weights-improvement-02-2.9993.hdf5\n",
      "Epoch 3/500\n",
      "47472/47472 [==============================] - 42s 893us/step - loss: 2.9892\n",
      "\n",
      "Epoch 00003: loss improved from 2.99927 to 2.98925, saving model to Bigger_Model_weights-improvement-03-2.9892.hdf5\n",
      "Epoch 4/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 2.9792\n",
      "\n",
      "Epoch 00004: loss improved from 2.98925 to 2.97921, saving model to Bigger_Model_weights-improvement-04-2.9792.hdf5\n",
      "Epoch 5/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 2.9692\n",
      "\n",
      "Epoch 00005: loss improved from 2.97921 to 2.96918, saving model to Bigger_Model_weights-improvement-05-2.9692.hdf5\n",
      "Epoch 6/500\n",
      "47472/47472 [==============================] - 43s 906us/step - loss: 2.9657\n",
      "\n",
      "Epoch 00006: loss improved from 2.96918 to 2.96572, saving model to Bigger_Model_weights-improvement-06-2.9657.hdf5\n",
      "Epoch 7/500\n",
      "47472/47472 [==============================] - 43s 902us/step - loss: 2.9537\n",
      "\n",
      "Epoch 00007: loss improved from 2.96572 to 2.95367, saving model to Bigger_Model_weights-improvement-07-2.9537.hdf5\n",
      "Epoch 8/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.9472\n",
      "\n",
      "Epoch 00008: loss improved from 2.95367 to 2.94722, saving model to Bigger_Model_weights-improvement-08-2.9472.hdf5\n",
      "Epoch 9/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 2.9362\n",
      "\n",
      "Epoch 00009: loss improved from 2.94722 to 2.93616, saving model to Bigger_Model_weights-improvement-09-2.9362.hdf5\n",
      "Epoch 10/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 2.9276\n",
      "\n",
      "Epoch 00010: loss improved from 2.93616 to 2.92756, saving model to Bigger_Model_weights-improvement-10-2.9276.hdf5\n",
      "Epoch 11/500\n",
      "47472/47472 [==============================] - 43s 899us/step - loss: 2.9157\n",
      "\n",
      "Epoch 00011: loss improved from 2.92756 to 2.91573, saving model to Bigger_Model_weights-improvement-11-2.9157.hdf5\n",
      "Epoch 12/500\n",
      "47472/47472 [==============================] - 43s 898us/step - loss: 2.9030\n",
      "\n",
      "Epoch 00012: loss improved from 2.91573 to 2.90296, saving model to Bigger_Model_weights-improvement-12-2.9030.hdf5\n",
      "Epoch 13/500\n",
      "47472/47472 [==============================] - 43s 908us/step - loss: 2.8925\n",
      "\n",
      "Epoch 00013: loss improved from 2.90296 to 2.89250, saving model to Bigger_Model_weights-improvement-13-2.8925.hdf5\n",
      "Epoch 14/500\n",
      "47472/47472 [==============================] - 43s 905us/step - loss: 2.8763\n",
      "\n",
      "Epoch 00014: loss improved from 2.89250 to 2.87630, saving model to Bigger_Model_weights-improvement-14-2.8763.hdf5\n",
      "Epoch 15/500\n",
      "47472/47472 [==============================] - 43s 902us/step - loss: 2.8599\n",
      "\n",
      "Epoch 00015: loss improved from 2.87630 to 2.85986, saving model to Bigger_Model_weights-improvement-15-2.8599.hdf5\n",
      "Epoch 16/500\n",
      "47472/47472 [==============================] - 43s 901us/step - loss: 2.8468\n",
      "\n",
      "Epoch 00016: loss improved from 2.85986 to 2.84678, saving model to Bigger_Model_weights-improvement-16-2.8468.hdf5\n",
      "Epoch 17/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.8289\n",
      "\n",
      "Epoch 00017: loss improved from 2.84678 to 2.82889, saving model to Bigger_Model_weights-improvement-17-2.8289.hdf5\n",
      "Epoch 18/500\n",
      "47472/47472 [==============================] - 43s 899us/step - loss: 2.8139\n",
      "\n",
      "Epoch 00018: loss improved from 2.82889 to 2.81386, saving model to Bigger_Model_weights-improvement-18-2.8139.hdf5\n",
      "Epoch 19/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 2.7914\n",
      "\n",
      "Epoch 00019: loss improved from 2.81386 to 2.79144, saving model to Bigger_Model_weights-improvement-19-2.7914.hdf5\n",
      "Epoch 20/500\n",
      "47472/47472 [==============================] - 42s 893us/step - loss: 2.7696\n",
      "\n",
      "Epoch 00020: loss improved from 2.79144 to 2.76963, saving model to Bigger_Model_weights-improvement-20-2.7696.hdf5\n",
      "Epoch 21/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.7473\n",
      "\n",
      "Epoch 00021: loss improved from 2.76963 to 2.74729, saving model to Bigger_Model_weights-improvement-21-2.7473.hdf5\n",
      "Epoch 22/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.7300\n",
      "\n",
      "Epoch 00022: loss improved from 2.74729 to 2.73004, saving model to Bigger_Model_weights-improvement-22-2.7300.hdf5\n",
      "Epoch 23/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.7044\n",
      "\n",
      "Epoch 00023: loss improved from 2.73004 to 2.70441, saving model to Bigger_Model_weights-improvement-23-2.7044.hdf5\n",
      "Epoch 24/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.6803\n",
      "\n",
      "Epoch 00024: loss improved from 2.70441 to 2.68028, saving model to Bigger_Model_weights-improvement-24-2.6803.hdf5\n",
      "Epoch 25/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.6536\n",
      "\n",
      "Epoch 00025: loss improved from 2.68028 to 2.65364, saving model to Bigger_Model_weights-improvement-25-2.6536.hdf5\n",
      "Epoch 26/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 2.6279\n",
      "\n",
      "Epoch 00026: loss improved from 2.65364 to 2.62785, saving model to Bigger_Model_weights-improvement-26-2.6279.hdf5\n",
      "Epoch 27/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 2.5999\n",
      "\n",
      "Epoch 00027: loss improved from 2.62785 to 2.59990, saving model to Bigger_Model_weights-improvement-27-2.5999.hdf5\n",
      "Epoch 28/500\n",
      "47472/47472 [==============================] - 43s 898us/step - loss: 2.5743\n",
      "\n",
      "Epoch 00028: loss improved from 2.59990 to 2.57428, saving model to Bigger_Model_weights-improvement-28-2.5743.hdf5\n",
      "Epoch 29/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 2.5455\n",
      "\n",
      "Epoch 00029: loss improved from 2.57428 to 2.54546, saving model to Bigger_Model_weights-improvement-29-2.5455.hdf5\n",
      "Epoch 30/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 2.5192\n",
      "\n",
      "Epoch 00030: loss improved from 2.54546 to 2.51925, saving model to Bigger_Model_weights-improvement-30-2.5192.hdf5\n",
      "Epoch 31/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.4923\n",
      "\n",
      "Epoch 00031: loss improved from 2.51925 to 2.49228, saving model to Bigger_Model_weights-improvement-31-2.4923.hdf5\n",
      "Epoch 32/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.4637\n",
      "\n",
      "Epoch 00032: loss improved from 2.49228 to 2.46367, saving model to Bigger_Model_weights-improvement-32-2.4637.hdf5\n",
      "Epoch 33/500\n",
      "47472/47472 [==============================] - 43s 898us/step - loss: 2.4268\n",
      "\n",
      "Epoch 00033: loss improved from 2.46367 to 2.42675, saving model to Bigger_Model_weights-improvement-33-2.4268.hdf5\n",
      "Epoch 34/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.3968\n",
      "\n",
      "Epoch 00034: loss improved from 2.42675 to 2.39678, saving model to Bigger_Model_weights-improvement-34-2.3968.hdf5\n",
      "Epoch 35/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.3758\n",
      "\n",
      "Epoch 00035: loss improved from 2.39678 to 2.37582, saving model to Bigger_Model_weights-improvement-35-2.3758.hdf5\n",
      "Epoch 36/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 2.3400\n",
      "\n",
      "Epoch 00036: loss improved from 2.37582 to 2.33999, saving model to Bigger_Model_weights-improvement-36-2.3400.hdf5\n",
      "Epoch 37/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.3166\n",
      "\n",
      "Epoch 00037: loss improved from 2.33999 to 2.31658, saving model to Bigger_Model_weights-improvement-37-2.3166.hdf5\n",
      "Epoch 38/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.2801\n",
      "\n",
      "Epoch 00038: loss improved from 2.31658 to 2.28014, saving model to Bigger_Model_weights-improvement-38-2.2801.hdf5\n",
      "Epoch 39/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 2.2530\n",
      "\n",
      "Epoch 00039: loss improved from 2.28014 to 2.25297, saving model to Bigger_Model_weights-improvement-39-2.2530.hdf5\n",
      "Epoch 40/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 2.2217\n",
      "\n",
      "Epoch 00040: loss improved from 2.25297 to 2.22173, saving model to Bigger_Model_weights-improvement-40-2.2217.hdf5\n",
      "Epoch 41/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47472/47472 [==============================] - 42s 891us/step - loss: 2.1979\n",
      "\n",
      "Epoch 00041: loss improved from 2.22173 to 2.19785, saving model to Bigger_Model_weights-improvement-41-2.1979.hdf5\n",
      "Epoch 42/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.1716\n",
      "\n",
      "Epoch 00042: loss improved from 2.19785 to 2.17157, saving model to Bigger_Model_weights-improvement-42-2.1716.hdf5\n",
      "Epoch 43/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 2.1414\n",
      "\n",
      "Epoch 00043: loss improved from 2.17157 to 2.14135, saving model to Bigger_Model_weights-improvement-43-2.1414.hdf5\n",
      "Epoch 44/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 2.1143\n",
      "\n",
      "Epoch 00044: loss improved from 2.14135 to 2.11434, saving model to Bigger_Model_weights-improvement-44-2.1143.hdf5\n",
      "Epoch 45/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 2.0801\n",
      "\n",
      "Epoch 00045: loss improved from 2.11434 to 2.08011, saving model to Bigger_Model_weights-improvement-45-2.0801.hdf5\n",
      "Epoch 46/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 2.0610\n",
      "\n",
      "Epoch 00046: loss improved from 2.08011 to 2.06098, saving model to Bigger_Model_weights-improvement-46-2.0610.hdf5\n",
      "Epoch 47/500\n",
      "47472/47472 [==============================] - 42s 893us/step - loss: 2.0275\n",
      "\n",
      "Epoch 00047: loss improved from 2.06098 to 2.02751, saving model to Bigger_Model_weights-improvement-47-2.0275.hdf5\n",
      "Epoch 48/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 1.9999\n",
      "\n",
      "Epoch 00048: loss improved from 2.02751 to 1.99990, saving model to Bigger_Model_weights-improvement-48-1.9999.hdf5\n",
      "Epoch 49/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.9809\n",
      "\n",
      "Epoch 00049: loss improved from 1.99990 to 1.98092, saving model to Bigger_Model_weights-improvement-49-1.9809.hdf5\n",
      "Epoch 50/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.9500\n",
      "\n",
      "Epoch 00050: loss improved from 1.98092 to 1.95001, saving model to Bigger_Model_weights-improvement-50-1.9500.hdf5\n",
      "Epoch 51/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 1.9331\n",
      "\n",
      "Epoch 00051: loss improved from 1.95001 to 1.93315, saving model to Bigger_Model_weights-improvement-51-1.9331.hdf5\n",
      "Epoch 52/500\n",
      "47472/47472 [==============================] - 43s 899us/step - loss: 1.9020\n",
      "\n",
      "Epoch 00052: loss improved from 1.93315 to 1.90203, saving model to Bigger_Model_weights-improvement-52-1.9020.hdf5\n",
      "Epoch 53/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.8792\n",
      "\n",
      "Epoch 00053: loss improved from 1.90203 to 1.87924, saving model to Bigger_Model_weights-improvement-53-1.8792.hdf5\n",
      "Epoch 54/500\n",
      "47472/47472 [==============================] - 43s 895us/step - loss: 1.8529\n",
      "\n",
      "Epoch 00054: loss improved from 1.87924 to 1.85295, saving model to Bigger_Model_weights-improvement-54-1.8529.hdf5\n",
      "Epoch 55/500\n",
      "47472/47472 [==============================] - 43s 903us/step - loss: 1.8251\n",
      "\n",
      "Epoch 00055: loss improved from 1.85295 to 1.82507, saving model to Bigger_Model_weights-improvement-55-1.8251.hdf5\n",
      "Epoch 56/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 1.8035\n",
      "\n",
      "Epoch 00056: loss improved from 1.82507 to 1.80349, saving model to Bigger_Model_weights-improvement-56-1.8035.hdf5\n",
      "Epoch 57/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.7894\n",
      "\n",
      "Epoch 00057: loss improved from 1.80349 to 1.78939, saving model to Bigger_Model_weights-improvement-57-1.7894.hdf5\n",
      "Epoch 58/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.7653\n",
      "\n",
      "Epoch 00058: loss improved from 1.78939 to 1.76532, saving model to Bigger_Model_weights-improvement-58-1.7653.hdf5\n",
      "Epoch 59/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.7482\n",
      "\n",
      "Epoch 00059: loss improved from 1.76532 to 1.74816, saving model to Bigger_Model_weights-improvement-59-1.7482.hdf5\n",
      "Epoch 60/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 1.7216\n",
      "\n",
      "Epoch 00060: loss improved from 1.74816 to 1.72159, saving model to Bigger_Model_weights-improvement-60-1.7216.hdf5\n",
      "Epoch 61/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.7071\n",
      "\n",
      "Epoch 00061: loss improved from 1.72159 to 1.70706, saving model to Bigger_Model_weights-improvement-61-1.7071.hdf5\n",
      "Epoch 62/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.6808\n",
      "\n",
      "Epoch 00062: loss improved from 1.70706 to 1.68079, saving model to Bigger_Model_weights-improvement-62-1.6808.hdf5\n",
      "Epoch 63/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.6715\n",
      "\n",
      "Epoch 00063: loss improved from 1.68079 to 1.67149, saving model to Bigger_Model_weights-improvement-63-1.6715.hdf5\n",
      "Epoch 64/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 1.6414\n",
      "\n",
      "Epoch 00064: loss improved from 1.67149 to 1.64143, saving model to Bigger_Model_weights-improvement-64-1.6414.hdf5\n",
      "Epoch 65/500\n",
      "47472/47472 [==============================] - 43s 899us/step - loss: 1.6250\n",
      "\n",
      "Epoch 00065: loss improved from 1.64143 to 1.62503, saving model to Bigger_Model_weights-improvement-65-1.6250.hdf5\n",
      "Epoch 66/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.6055\n",
      "\n",
      "Epoch 00066: loss improved from 1.62503 to 1.60547, saving model to Bigger_Model_weights-improvement-66-1.6055.hdf5\n",
      "Epoch 67/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.5886\n",
      "\n",
      "Epoch 00067: loss improved from 1.60547 to 1.58857, saving model to Bigger_Model_weights-improvement-67-1.5886.hdf5\n",
      "Epoch 68/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 1.5757\n",
      "\n",
      "Epoch 00068: loss improved from 1.58857 to 1.57569, saving model to Bigger_Model_weights-improvement-68-1.5757.hdf5\n",
      "Epoch 69/500\n",
      "47472/47472 [==============================] - 42s 893us/step - loss: 1.5564\n",
      "\n",
      "Epoch 00069: loss improved from 1.57569 to 1.55637, saving model to Bigger_Model_weights-improvement-69-1.5564.hdf5\n",
      "Epoch 70/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 1.5471\n",
      "\n",
      "Epoch 00070: loss improved from 1.55637 to 1.54714, saving model to Bigger_Model_weights-improvement-70-1.5471.hdf5\n",
      "Epoch 71/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 1.5242\n",
      "\n",
      "Epoch 00071: loss improved from 1.54714 to 1.52423, saving model to Bigger_Model_weights-improvement-71-1.5242.hdf5\n",
      "Epoch 72/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 1.5141\n",
      "\n",
      "Epoch 00072: loss improved from 1.52423 to 1.51406, saving model to Bigger_Model_weights-improvement-72-1.5141.hdf5\n",
      "Epoch 73/500\n",
      "47472/47472 [==============================] - 43s 898us/step - loss: 1.4844\n",
      "\n",
      "Epoch 00073: loss improved from 1.51406 to 1.48442, saving model to Bigger_Model_weights-improvement-73-1.4844.hdf5\n",
      "Epoch 74/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.4716\n",
      "\n",
      "Epoch 00074: loss improved from 1.48442 to 1.47157, saving model to Bigger_Model_weights-improvement-74-1.4716.hdf5\n",
      "Epoch 75/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.4641\n",
      "\n",
      "Epoch 00075: loss improved from 1.47157 to 1.46414, saving model to Bigger_Model_weights-improvement-75-1.4641.hdf5\n",
      "Epoch 76/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 1.4500\n",
      "\n",
      "Epoch 00076: loss improved from 1.46414 to 1.44996, saving model to Bigger_Model_weights-improvement-76-1.4500.hdf5\n",
      "Epoch 77/500\n",
      "47472/47472 [==============================] - 43s 898us/step - loss: 1.4353\n",
      "\n",
      "Epoch 00077: loss improved from 1.44996 to 1.43532, saving model to Bigger_Model_weights-improvement-77-1.4353.hdf5\n",
      "Epoch 78/500\n",
      "47472/47472 [==============================] - 42s 888us/step - loss: 1.4224\n",
      "\n",
      "Epoch 00078: loss improved from 1.43532 to 1.42237, saving model to Bigger_Model_weights-improvement-78-1.4224.hdf5\n",
      "Epoch 79/500\n",
      "47472/47472 [==============================] - 42s 890us/step - loss: 1.4163\n",
      "\n",
      "Epoch 00079: loss improved from 1.42237 to 1.41630, saving model to Bigger_Model_weights-improvement-79-1.4163.hdf5\n",
      "Epoch 80/500\n",
      "47472/47472 [==============================] - 42s 889us/step - loss: 1.3837\n",
      "\n",
      "Epoch 00080: loss improved from 1.41630 to 1.38368, saving model to Bigger_Model_weights-improvement-80-1.3837.hdf5\n",
      "Epoch 81/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47472/47472 [==============================] - 42s 887us/step - loss: 1.3769\n",
      "\n",
      "Epoch 00081: loss improved from 1.38368 to 1.37686, saving model to Bigger_Model_weights-improvement-81-1.3769.hdf5\n",
      "Epoch 82/500\n",
      "47472/47472 [==============================] - 42s 890us/step - loss: 1.3501\n",
      "\n",
      "Epoch 00082: loss improved from 1.37686 to 1.35008, saving model to Bigger_Model_weights-improvement-82-1.3501.hdf5\n",
      "Epoch 83/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.3419\n",
      "\n",
      "Epoch 00083: loss improved from 1.35008 to 1.34192, saving model to Bigger_Model_weights-improvement-83-1.3419.hdf5\n",
      "Epoch 84/500\n",
      "47472/47472 [==============================] - 42s 889us/step - loss: 1.3359\n",
      "\n",
      "Epoch 00084: loss improved from 1.34192 to 1.33594, saving model to Bigger_Model_weights-improvement-84-1.3359.hdf5\n",
      "Epoch 85/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 1.3322\n",
      "\n",
      "Epoch 00085: loss improved from 1.33594 to 1.33219, saving model to Bigger_Model_weights-improvement-85-1.3322.hdf5\n",
      "Epoch 86/500\n",
      "47472/47472 [==============================] - 42s 889us/step - loss: 1.3118\n",
      "\n",
      "Epoch 00086: loss improved from 1.33219 to 1.31176, saving model to Bigger_Model_weights-improvement-86-1.3118.hdf5\n",
      "Epoch 87/500\n",
      "47472/47472 [==============================] - 42s 893us/step - loss: 1.3120\n",
      "\n",
      "Epoch 00087: loss did not improve from 1.31176\n",
      "Epoch 88/500\n",
      "47472/47472 [==============================] - 43s 900us/step - loss: 1.2825\n",
      "\n",
      "Epoch 00088: loss improved from 1.31176 to 1.28245, saving model to Bigger_Model_weights-improvement-88-1.2825.hdf5\n",
      "Epoch 89/500\n",
      "47472/47472 [==============================] - 43s 898us/step - loss: 1.2695\n",
      "\n",
      "Epoch 00089: loss improved from 1.28245 to 1.26951, saving model to Bigger_Model_weights-improvement-89-1.2695.hdf5\n",
      "Epoch 90/500\n",
      "47472/47472 [==============================] - 42s 892us/step - loss: 1.2701\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.26951\n",
      "Epoch 91/500\n",
      "47472/47472 [==============================] - 42s 891us/step - loss: 1.2438\n",
      "\n",
      "Epoch 00091: loss improved from 1.26951 to 1.24378, saving model to Bigger_Model_weights-improvement-91-1.2438.hdf5\n",
      "Epoch 92/500\n",
      "47472/47472 [==============================] - 42s 889us/step - loss: 1.2472\n",
      "\n",
      "Epoch 00092: loss did not improve from 1.24378\n",
      "Epoch 93/500\n",
      "47472/47472 [==============================] - 43s 902us/step - loss: 1.2240\n",
      "\n",
      "Epoch 00093: loss improved from 1.24378 to 1.22401, saving model to Bigger_Model_weights-improvement-93-1.2240.hdf5\n",
      "Epoch 94/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.2242\n",
      "\n",
      "Epoch 00094: loss did not improve from 1.22401\n",
      "Epoch 95/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.2085\n",
      "\n",
      "Epoch 00095: loss improved from 1.22401 to 1.20847, saving model to Bigger_Model_weights-improvement-95-1.2085.hdf5\n",
      "Epoch 96/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 1.2025\n",
      "\n",
      "Epoch 00096: loss improved from 1.20847 to 1.20252, saving model to Bigger_Model_weights-improvement-96-1.2025.hdf5\n",
      "Epoch 97/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 1.1965\n",
      "\n",
      "Epoch 00097: loss improved from 1.20252 to 1.19650, saving model to Bigger_Model_weights-improvement-97-1.1965.hdf5\n",
      "Epoch 98/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.1801\n",
      "\n",
      "Epoch 00098: loss improved from 1.19650 to 1.18011, saving model to Bigger_Model_weights-improvement-98-1.1801.hdf5\n",
      "Epoch 99/500\n",
      "47472/47472 [==============================] - 43s 900us/step - loss: 1.1577\n",
      "\n",
      "Epoch 00099: loss improved from 1.18011 to 1.15775, saving model to Bigger_Model_weights-improvement-99-1.1577.hdf5\n",
      "Epoch 100/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.1639\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.15775\n",
      "Epoch 101/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.1467\n",
      "\n",
      "Epoch 00101: loss improved from 1.15775 to 1.14672, saving model to Bigger_Model_weights-improvement-101-1.1467.hdf5\n",
      "Epoch 102/500\n",
      "47472/47472 [==============================] - 43s 895us/step - loss: 1.1535\n",
      "\n",
      "Epoch 00102: loss did not improve from 1.14672\n",
      "Epoch 103/500\n",
      "47472/47472 [==============================] - 43s 900us/step - loss: 1.1336\n",
      "\n",
      "Epoch 00103: loss improved from 1.14672 to 1.13356, saving model to Bigger_Model_weights-improvement-103-1.1336.hdf5\n",
      "Epoch 104/500\n",
      "47472/47472 [==============================] - 43s 901us/step - loss: 1.1217\n",
      "\n",
      "Epoch 00104: loss improved from 1.13356 to 1.12167, saving model to Bigger_Model_weights-improvement-104-1.1217.hdf5\n",
      "Epoch 105/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.1220\n",
      "\n",
      "Epoch 00105: loss did not improve from 1.12167\n",
      "Epoch 106/500\n",
      "47472/47472 [==============================] - 43s 898us/step - loss: 1.1128\n",
      "\n",
      "Epoch 00106: loss improved from 1.12167 to 1.11281, saving model to Bigger_Model_weights-improvement-106-1.1128.hdf5\n",
      "Epoch 107/500\n",
      "47472/47472 [==============================] - 43s 900us/step - loss: 1.1032\n",
      "\n",
      "Epoch 00107: loss improved from 1.11281 to 1.10324, saving model to Bigger_Model_weights-improvement-107-1.1032.hdf5\n",
      "Epoch 108/500\n",
      "47472/47472 [==============================] - 43s 909us/step - loss: 1.0878\n",
      "\n",
      "Epoch 00108: loss improved from 1.10324 to 1.08775, saving model to Bigger_Model_weights-improvement-108-1.0878.hdf5\n",
      "Epoch 109/500\n",
      "47472/47472 [==============================] - 43s 905us/step - loss: 1.0877\n",
      "\n",
      "Epoch 00109: loss improved from 1.08775 to 1.08771, saving model to Bigger_Model_weights-improvement-109-1.0877.hdf5\n",
      "Epoch 110/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.0684\n",
      "\n",
      "Epoch 00110: loss improved from 1.08771 to 1.06837, saving model to Bigger_Model_weights-improvement-110-1.0684.hdf5\n",
      "Epoch 111/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.0690\n",
      "\n",
      "Epoch 00111: loss did not improve from 1.06837\n",
      "Epoch 112/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.0619\n",
      "\n",
      "Epoch 00112: loss improved from 1.06837 to 1.06188, saving model to Bigger_Model_weights-improvement-112-1.0619.hdf5\n",
      "Epoch 113/500\n",
      "47472/47472 [==============================] - 43s 899us/step - loss: 1.0530\n",
      "\n",
      "Epoch 00113: loss improved from 1.06188 to 1.05301, saving model to Bigger_Model_weights-improvement-113-1.0530.hdf5\n",
      "Epoch 114/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 1.0358\n",
      "\n",
      "Epoch 00114: loss improved from 1.05301 to 1.03580, saving model to Bigger_Model_weights-improvement-114-1.0358.hdf5\n",
      "Epoch 115/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 1.0270\n",
      "\n",
      "Epoch 00115: loss improved from 1.03580 to 1.02704, saving model to Bigger_Model_weights-improvement-115-1.0270.hdf5\n",
      "Epoch 116/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.0368\n",
      "\n",
      "Epoch 00116: loss did not improve from 1.02704\n",
      "Epoch 117/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 1.0424\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.02704\n",
      "Epoch 118/500\n",
      "47472/47472 [==============================] - 43s 895us/step - loss: 1.0307\n",
      "\n",
      "Epoch 00118: loss did not improve from 1.02704\n",
      "Epoch 119/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 1.0168\n",
      "\n",
      "Epoch 00119: loss improved from 1.02704 to 1.01677, saving model to Bigger_Model_weights-improvement-119-1.0168.hdf5\n",
      "Epoch 120/500\n",
      "47472/47472 [==============================] - 43s 897us/step - loss: 1.0082\n",
      "\n",
      "Epoch 00120: loss improved from 1.01677 to 1.00816, saving model to Bigger_Model_weights-improvement-120-1.0082.hdf5\n",
      "Epoch 121/500\n",
      "47472/47472 [==============================] - 43s 895us/step - loss: 0.9896\n",
      "\n",
      "Epoch 00121: loss improved from 1.00816 to 0.98963, saving model to Bigger_Model_weights-improvement-121-0.9896.hdf5\n",
      "Epoch 122/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 0.9889\n",
      "\n",
      "Epoch 00122: loss improved from 0.98963 to 0.98892, saving model to Bigger_Model_weights-improvement-122-0.9889.hdf5\n",
      "Epoch 123/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 0.9816\n",
      "\n",
      "Epoch 00123: loss improved from 0.98892 to 0.98160, saving model to Bigger_Model_weights-improvement-123-0.9816.hdf5\n",
      "Epoch 124/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47472/47472 [==============================] - 42s 893us/step - loss: 0.9731\n",
      "\n",
      "Epoch 00124: loss improved from 0.98160 to 0.97308, saving model to Bigger_Model_weights-improvement-124-0.9731.hdf5\n",
      "Epoch 125/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 0.9770\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.97308\n",
      "Epoch 126/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 0.9613\n",
      "\n",
      "Epoch 00126: loss improved from 0.97308 to 0.96127, saving model to Bigger_Model_weights-improvement-126-0.9613.hdf5\n",
      "Epoch 127/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 0.9558\n",
      "\n",
      "Epoch 00127: loss improved from 0.96127 to 0.95583, saving model to Bigger_Model_weights-improvement-127-0.9558.hdf5\n",
      "Epoch 128/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 0.9464\n",
      "\n",
      "Epoch 00128: loss improved from 0.95583 to 0.94640, saving model to Bigger_Model_weights-improvement-128-0.9464.hdf5\n",
      "Epoch 129/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 0.9502\n",
      "\n",
      "Epoch 00129: loss did not improve from 0.94640\n",
      "Epoch 130/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 0.9280\n",
      "\n",
      "Epoch 00130: loss improved from 0.94640 to 0.92801, saving model to Bigger_Model_weights-improvement-130-0.9280.hdf5\n",
      "Epoch 131/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 0.9488\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.92801\n",
      "Epoch 132/500\n",
      "47472/47472 [==============================] - 43s 898us/step - loss: 0.9234\n",
      "\n",
      "Epoch 00132: loss improved from 0.92801 to 0.92345, saving model to Bigger_Model_weights-improvement-132-0.9234.hdf5\n",
      "Epoch 133/500\n",
      "47472/47472 [==============================] - 42s 894us/step - loss: 0.9162\n",
      "\n",
      "Epoch 00133: loss improved from 0.92345 to 0.91625, saving model to Bigger_Model_weights-improvement-133-0.9162.hdf5\n",
      "Epoch 134/500\n",
      "47472/47472 [==============================] - 43s 898us/step - loss: 0.9089\n",
      "\n",
      "Epoch 00134: loss improved from 0.91625 to 0.90888, saving model to Bigger_Model_weights-improvement-134-0.9089.hdf5\n",
      "Epoch 135/500\n",
      "47472/47472 [==============================] - 43s 896us/step - loss: 0.9047\n",
      "\n",
      "Epoch 00135: loss improved from 0.90888 to 0.90475, saving model to Bigger_Model_weights-improvement-135-0.9047.hdf5\n",
      "Epoch 136/500\n",
      "47472/47472 [==============================] - 42s 895us/step - loss: 0.8978\n",
      "\n",
      "Epoch 00136: loss improved from 0.90475 to 0.89776, saving model to Bigger_Model_weights-improvement-136-0.8978.hdf5\n",
      "Epoch 137/500\n",
      "47472/47472 [==============================] - 43s 895us/step - loss: 0.8915\n",
      "\n",
      "Epoch 00137: loss improved from 0.89776 to 0.89146, saving model to Bigger_Model_weights-improvement-137-0.8915.hdf5\n",
      "Epoch 138/500\n",
      "13824/47472 [=======>......................] - ETA: 30s - loss: 0.8400"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-a1c12cd4beaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\dhruv rawat\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\dhruv rawat\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dhruv rawat\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dhruv rawat\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dhruv rawat\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=500, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Passwords from the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"Weights/FreazedWeights/Bigger_Model_weights-improvement-137-0.8915.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" nis\n",
      "rockstar\n",
      "password\n",
      "123 \"\n",
      "testandok\n",
      "canaeiim1\n",
      "m17221997\n",
      "anigma89\n",
      "ricogor\n",
      "gicfeaooa\n",
      "bllln1\n",
      "lobaby\n",
      "landna\n",
      "dorlinas12\n",
      "f1com389\n",
      "dollplan\n",
      "lillan12\n",
      "shinanaki1\n",
      "songmo19\n",
      "Kallyeder3\n",
      "sagoldom11\n",
      "lorwoa\n",
      "joosyt162\n",
      "lelk2222\n",
      "fotena12\n",
      "laiecan\n",
      "kust5lite\n",
      "srtpun\n",
      "sskken21\n",
      "tadrer11\n",
      "shantw123\n",
      "lala11\n",
      "S123052\n",
      "00000000\n",
      "Shhwee123\n",
      "izotnana2\n",
      "janiysoy88\n",
      "bllllt00\n",
      "shahiw01\n",
      "sacast\n",
      "723412\n",
      "coanoan\n",
      "smhoshn\n",
      "clefsb17\n",
      "milkeaho\n",
      "lokenon100\n",
      "123312\n",
      "tik2600\n",
      "Drggougar3\n",
      "seekelo17\n",
      "ciboiet1\n",
      "0897899\n",
      "po5rswrn\n",
      "ph020216li\n",
      "1234\n",
      "slarey1\n",
      "tevenan1\n",
      "noo15001\n",
      "samohn\n",
      "bllddy11\n",
      "marsanama\n",
      "259853\n",
      "viiyaie\n",
      "steter12\n",
      "geananakt12\n",
      "adsx12\n",
      "bumger\n",
      "12041971a\n",
      "591885556\n",
      "123456\n",
      "qhageen15\n",
      "sioooiipp\n",
      "damunono\n",
      "niovir\n",
      "DLSHDTFER\n",
      "recsis\n",
      "cccuaoed\n",
      "dilpel2\n",
      "mastaryo1\n",
      "kifnta\n",
      "iayhan\n",
      "ticrisss\n",
      "joefoel1\n",
      "123973\n",
      "230459\n",
      "pobhty1\n",
      "lhruia\n",
      "Iarswee1\n",
      "bokkoa02\n",
      "hragss100\n",
      "maiiii12\n",
      "skh153456\n",
      "maraoengen\n",
      "muduls12\n",
      "shoninani12330\n",
      "11341678\n",
      "aragtang96\n",
      "230019sa\n",
      "lle223456\n",
      "mmbdue11\n",
      "maciia\n",
      "120453\n",
      "eeant1\n",
      "aofnsgat\n",
      "elesacas6\n",
      "a1cdooort\n",
      "sohhon\n",
      "mogkaodz\n",
      "pplmlay1\n",
      "iriosenar1\n",
      "123456789\n",
      "higt20\n",
      "mhogun\n",
      "12342678\n",
      "nct1997\n",
      "928oon\n",
      "wcnk3895\n",
      "f\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "#pick a random seed\n",
    "start = numpy.random.randint(0,100)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
